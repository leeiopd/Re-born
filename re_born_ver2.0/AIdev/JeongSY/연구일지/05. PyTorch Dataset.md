## PyTorch Dataset

전체 데이터를 균일하게 나눠서 학습해요. 

실제 세계에서는 엄청난 양의 데이터가 있으며, 이를 일부분의 데이터로만 학습하는 거에요

![minibatch](./images/minibatch.png)

![minibatch2](./images/minibatch2.png)

전체 데이터를 쓰지 않아서 일반적인 cost 처럼 매끄럽게 내려오는 것이아닌 거칠게 내려와요.




```python
from torch.utils.data import Dataset

class CustomDataset(Dataset):
    def __init__(self):
        self.x_data = [[73, 80, 75], [93, 88, 93], [89, 91, 90], [96, 98, 100], [73, 66, 70]]
        self.y_data = [[152],[185],[180],[196],[142]]
    
    def __len__(self):
        return len(self.x_data)
    
    def __getitem__(self, idx):
        x = torch.FloatTensor(self.x_data[idx])
        y = torch.FloatTensor(self.y_data[idx])
        
        return x, y

dataset = CustomDataset()
```

```python
from torch.utils.data import DataLoader

dataloader = DataLoader(
    dataset,
    batch_size = 2,
    shuffle = True,
)
```

* batch_size = 2
  * 각 minibatch의 크기
  * 통상적으로 2의 제곱수로 설정해요 (16,32,64, ....)
* shuffle = True
  * Epoch 마다 데이터셋을 섞어서, 데이터가 학습되는 순서를 바꾼다.



## Full Code

```python
nb_epochs = 20
for epoch in range(nb_epochs + 1):
    for batch_idx, samples in enumerate(dataloader):
        x_train, y_train = samples
        
        prediction = model(x_train)
        
        cost = F.mse_loss(prediction, y_train)
        
        optimizer.zero_grad()
        cost.backward()
        optimizer.step()
        
        print('Epoch {:4d}/{}, Batch {}/{}, Cost: {:.6f}'.format(epoch, nb_epochs,batch_idx+1,len(dataloader),cost.item()))
```



### 지끔까지는 어떠한 숫자 하나를 예측하는 모델을 만들었어요. 그럼 분류하는 모델은 다음번에

